{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fb8ddd07-7c8d-42fa-9fa8-b15443f8301f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ---------- CNPJ (ZIP -> Volume -> Delta/UC) ----------\n",
    "ensure_catalog_schema(raw_catalog, raw_schema_cnpj)\n",
    "\n",
    "# garantir pastas no Volume\n",
    "dbutils.fs.mkdirs(f\"{vol_dbfs_base}/zips\")\n",
    "dbutils.fs.mkdirs(f\"{vol_dbfs_base}/extract\")\n",
    "\n",
    "zip_files = [f for f in list_dir(src_cnpj_dir) if f.name.lower().endswith(\".zip\")]\n",
    "\n",
    "for f in zip_files:\n",
    "    base_no_ext = f.name[:-4]\n",
    "    fqn_table   = f\"`{raw_catalog}`.`{raw_schema_cnpj}`.`{base_no_ext}`\"\n",
    "    delta_dir   = f\"{raw_base}/cnpj/{base_no_ext}\"\n",
    "\n",
    "    zip_dbfs_dst   = f\"{vol_dbfs_base}/zips/{f.name}\"\n",
    "    zip_local_dst  = f\"{vol_local_base}/zips/{f.name}\"\n",
    "    extract_local  = f\"{vol_local_base}/extract/{base_no_ext}\"\n",
    "    extract_dbfs   = f\"{vol_dbfs_base}/extract/{base_no_ext}\"  # para spark.read\n",
    "\n",
    "    print(f\"\\n==> ZIP: {f.path}\")\n",
    "    print(\"    staging zip  :\", zip_dbfs_dst)\n",
    "    print(\"    extract local:\", extract_local)\n",
    "    print(\"    delta        :\", delta_dir)\n",
    "    print(\"    tabela       :\", fqn_table)\n",
    "\n",
    "    # 1) Copiar ZIP para o Volume (staging)\n",
    "    if dry_run:\n",
    "        print(f\"[DRY-RUN] cp {f.path} -> {zip_dbfs_dst}\")\n",
    "    else:\n",
    "        dbutils.fs.cp(f.path, zip_dbfs_dst, recurse=False)\n",
    "\n",
    "    # 2) Unzip dentro do Volume (POSIX /Volumes/…)\n",
    "    if dry_run:\n",
    "        print(f\"[DRY-RUN] unzip {zip_local_dst} -> {extract_local}\")\n",
    "    else:\n",
    "        # limpar extração anterior\n",
    "        if os.path.exists(extract_local):\n",
    "            shutil.rmtree(extract_local, ignore_errors=True)\n",
    "        os.makedirs(extract_local, exist_ok=True)\n",
    "        with zipfile.ZipFile(zip_local_dst, 'r') as z:\n",
    "            z.extractall(extract_local)\n",
    "\n",
    "    # 3) Ler CSVs extraídos (via dbfs:/Volumes/…) e gravar Delta em raw\n",
    "    if dry_run:\n",
    "        print(f\"[DRY-RUN] ler CSV(s) de {extract_dbfs}/**/*.csv\")\n",
    "        print(f\"[DRY-RUN] salvar Delta -> {delta_dir}\")\n",
    "        create_or_replace_external_delta_table(fqn_table, delta_dir)\n",
    "    else:\n",
    "        df = (spark.read\n",
    "                    .options(**csv_options)\n",
    "                    .option(\"recursiveFileLookup\", \"true\")\n",
    "                    .csv(f\"{extract_dbfs}\"))\n",
    "        (df.write\n",
    "            .format(\"delta\")\n",
    "            .mode(\"overwrite\")\n",
    "            .option(\"overwriteSchema\", \"true\")\n",
    "            .option(\"delta.columnMapping.mode\", \"name\")\n",
    "            .option(\"path\", delta_dir)              # importante\n",
    "            .saveAsTable(fqn_table))                # registra já com a propriedade"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Untitled Notebook 2026-02-03 14:15:12",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
