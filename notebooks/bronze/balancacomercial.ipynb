{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f3d92617-ae4e-4ec3-8e41-da37741da8e6",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Untitled"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import lit\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1e5fccb9-6c8b-47f2-ae29-542e4560cda2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "RAW_BASE    = \"abfss://balancacomercial@landingbeca2026jan.dfs.core.windows.net/\"\n",
    "BRONZE_BASE = \"abfss://bronze@storagedatanexus.dfs.core.windows.net/autoloader/landingbeca2026jan/balancacomercial/\"\n",
    "\n",
    "ENC         = \"latin1\"\n",
    "SEP         = \";\"\n",
    "MULTILINE   = \"true\"\n",
    "ESCAPE_CHAR = \"\\\\\"\n",
    "MAX_FILES   = \"1000\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "677336fe-50da-4036-882b-241491d14e40",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "arquivos = [f for f in dbutils.fs.ls(RAW_BASE) if not f.isDir()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "364c70ad-054a-4512-8688-6f7facd54c70",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Cell 4"
    }
   },
   "outputs": [],
   "source": [
    "for arq in arquivos:\n",
    "\n",
    "    fileName    = arq.name\n",
    "    tableName   = fileName.replace(\".csv\", \"\")\n",
    "    rawFilePath = RAW_BASE + fileName\n",
    "    bronzePath  = BRONZE_BASE + tableName + \"_delta\"\n",
    "    chkBase     = BRONZE_BASE + tableName + \"_chk\"\n",
    "\n",
    "    dbutils.fs.mkdirs(BRONZE_BASE)\n",
    "    dbutils.fs.mkdirs(chkBase)\n",
    "\n",
    "    print(f\"\\n=== PROCESSANDO: {fileName} ===\")\n",
    "    print(f\"RAW: {rawFilePath}\")\n",
    "    print(f\"BRONZE (tabela): bronze_balancacomercial.{tableName}\\n\")\n",
    "\n",
    "    reader = (\n",
    "        spark.readStream\n",
    "            .format(\"cloudFiles\")\n",
    "            .option(\"cloudFiles.format\", \"csv\")\n",
    "            .option(\"cloudFiles.schemaLocation\", f\"{chkBase}/_schema\")\n",
    "            .option(\"cloudFiles.inferColumnTypes\", \"true\")\n",
    "            .option(\"cloudFiles.includeExistingFiles\", \"true\")\n",
    "            .option(\"cloudFiles.maxFilesPerTrigger\", MAX_FILES)\n",
    "            .option(\"pathGlobFilter\", fileName)\n",
    "            .option(\"header\", \"true\")\n",
    "            .option(\"sep\", SEP)\n",
    "            .option(\"multiLine\", MULTILINE)\n",
    "            .option(\"escape\", ESCAPE_CHAR)\n",
    "            .option(\"encoding\", ENC)\n",
    "            .option(\"badRecordsPath\", f\"{chkBase}/_bad\")\n",
    "            .option(\"rescuedDataColumn\", \"_rescued\")\n",
    "            .load(RAW_BASE)\n",
    "    )\n",
    "\n",
    "    dfWithSource = reader.withColumn(\"SOURCE_FILE\", lit(fileName))\n",
    "\n",
    "    query = (\n",
    "        dfWithSource.writeStream\n",
    "            .format(\"delta\")\n",
    "            .option(\"checkpointLocation\", f\"{chkBase}/_checkpoint\")\n",
    "            .option(\"mergeSchema\", \"true\")\n",
    "            .outputMode(\"append\")\n",
    "            .trigger(availableNow=True)\n",
    "            .toTable(f\"bronze_balancacomercial.{tableName}\")\n",
    "    )\n",
    "\n",
    "    query.awaitTermination()\n",
    "\n",
    "    dfBronze = spark.table(f\"bronze_balancacomercial.{tableName}\")\n",
    "    print(f\"Linhas ingeridas para {tableName}: {dfBronze.count()}\")\n",
    "\n",
    "print(\"\\n=== INGESTÃO RAW → BRONZE FINALIZADA ===\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "balancacomercial",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
